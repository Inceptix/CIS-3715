{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8, Part 2:   Recurrent Neural Networks (RNN)  -- Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to model sequential data such as sentences, documents and videos, etc, the state of the art approach is to use Recurrent neural network (RNN). At each timestep, RNN takes an element (such as a word) as input, combines with past information encoded as a vector (such as all information in the sentence before this timestep), generate a new vector encoding both current input and past information, then delivers it to next timestep.\n",
    "\n",
    "For more details about LSTM (a very popular variant of RNN), please refer to http://colah.github.io/posts/2015-08-Understanding-LSTMs/ and here is a very good video explaining RNN: https://www.youtube.com/watch?v=WCUNPb-5EYI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with Long Short-Term Memory Networks\n",
    "\n",
    "RNN can be used to generate text. For more information, please read: https://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
    "\n",
    "The following is an example script to generate text from Nietzsche's writings.\n",
    "\n",
    "Note: \n",
    "- At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "\n",
    "- It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "\n",
    "- If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries \n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n",
      "total chars: 57\n"
     ]
    }
   ],
   "source": [
    "#Get the data - available from amazon\n",
    "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower() # make it all lowercase \n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 200285\n",
      "Vectorization...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cut the text in semi-redundant sequences of maxlen characters\n",
    "## Cut the text into a series of windows. \n",
    "## Each window is 40 characters\n",
    "## The window moves 3 steps forward each step\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# Turn these sentances into one-hot encoded vectors\n",
    "## For all words in the sentances, there is a one, else there is a zero in that index of the vector\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have data to feed a model for text generation. Next  we build a LSTM model to fit the data. Using Keras this is only few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (reduce the number of epochs, it takes a lot of time!!)\n",
    "-  Each epoch takes 5-10 minutes or so on a CPU (an epoch took 7.5 minutes for my PC)\n",
    "-  Recall that training on at least 20 epochs will give intelligible results \n",
    "-  So you're gonna have to let that puppy run for a while (2-3 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "200285/200285 [==============================] - 70s 350us/step - loss: 1.9728\n",
      "Epoch 2/25\n",
      "200285/200285 [==============================] - 66s 331us/step - loss: 1.6213\n",
      "Epoch 3/25\n",
      "200285/200285 [==============================] - 68s 339us/step - loss: 1.5328\n",
      "Epoch 4/25\n",
      "200285/200285 [==============================] - 66s 328us/step - loss: 1.4878\n",
      "Epoch 5/25\n",
      "200285/200285 [==============================] - 67s 335us/step - loss: 1.4585\n",
      "Epoch 6/25\n",
      "200285/200285 [==============================] - 66s 328us/step - loss: 1.4356\n",
      "Epoch 7/25\n",
      "200285/200285 [==============================] - 68s 341us/step - loss: 1.4206\n",
      "Epoch 8/25\n",
      "200285/200285 [==============================] - 75s 372us/step - loss: 1.4075\n",
      "Epoch 9/25\n",
      "200285/200285 [==============================] - 71s 356us/step - loss: 1.3954\n",
      "Epoch 10/25\n",
      "200285/200285 [==============================] - 73s 364us/step - loss: 1.3844\n",
      "Epoch 11/25\n",
      "200285/200285 [==============================] - 71s 356us/step - loss: 1.3753\n",
      "Epoch 12/25\n",
      "200285/200285 [==============================] - 72s 359us/step - loss: 1.3680\n",
      "Epoch 13/25\n",
      "200285/200285 [==============================] - 73s 365us/step - loss: 1.3618\n",
      "Epoch 14/25\n",
      "200285/200285 [==============================] - 74s 371us/step - loss: 1.3561\n",
      "Epoch 15/25\n",
      "200285/200285 [==============================] - 78s 387us/step - loss: 1.3487\n",
      "Epoch 16/25\n",
      "200285/200285 [==============================] - 78s 389us/step - loss: 1.3441\n",
      "Epoch 17/25\n",
      "200285/200285 [==============================] - 76s 381us/step - loss: 1.3394\n",
      "Epoch 18/25\n",
      "200285/200285 [==============================] - 73s 366us/step - loss: 1.3340\n",
      "Epoch 19/25\n",
      "200285/200285 [==============================] - 79s 394us/step - loss: 1.3291\n",
      "Epoch 20/25\n",
      "200285/200285 [==============================] - 79s 396us/step - loss: 1.3253\n",
      "Epoch 21/25\n",
      "200285/200285 [==============================] - 80s 397us/step - loss: 1.3228\n",
      "Epoch 22/25\n",
      "200285/200285 [==============================] - 79s 393us/step - loss: 1.3173\n",
      "Epoch 23/25\n",
      "200285/200285 [==============================] - 79s 395us/step - loss: 1.3145\n",
      "Epoch 24/25\n",
      "200285/200285 [==============================] - 84s 421us/step - loss: 1.3120\n",
      "Epoch 25/25\n",
      "200285/200285 [==============================] - 81s 406us/step - loss: 1.3082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17370d5bfd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "200285/200285 [==============================] - 75s 376us/step - loss: 1.3050\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"raftiness, with\n",
      "which the problem of \"th\"\n",
      "raftiness, with\n",
      "which the problem of \"the stands to be soul and believis of the stands to the substance of the sensation of the same time and all the sensation of a philosophers of the stands of his soul of the present of the same time who is a strong to the heart of the spirit and a superficial soul of the present of the stands to the sensation of the fact the consideration of probable and all the superstitions and all the same powerfu\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"raftiness, with\n",
      "which the problem of \"th\"\n",
      "raftiness, with\n",
      "which the problem of \"the spirit is the will to colus in the long present, and so intentiness of the present of his own conceing support, for uponthery subsiamed and desirable that as the dangerous problemsnes. it is also also been can be sole a thing, however, and the same most and preserion, and even into the passion of a men and one would the assemple person of great sign of attain to understood upon the process. it i\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"raftiness, with\n",
      "which the problem of \"th\"\n",
      "raftiness, with\n",
      "which the problem of \"the establilg bless a value and increases of the image silent in hidre--that who \"his \"prospous in prequie. proceed to ruse a state he of christian in the arrence does gaction in naive opinion of popselful anianism of beally live that they the world,\n",
      "who\n",
      "pleas is \"often is power. what so a such on\n",
      "the courage of determine ofe, a suge. what is just onlanscent\n",
      "that wale contracine, in a lewx for gowe \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"raftiness, with\n",
      "which the problem of \"th\"\n",
      "raftiness, with\n",
      "which the problem of \"thus herefore in a man\":--as i-heavch of notion of madatly\n",
      "can varyment at eutope, all, also\n",
      "there in ios\" arrages, overlowarise. the eara!teal around indeeded veoinary da. fratively\n",
      "powedonmentive\n",
      "rashers,\" to\n",
      "hitherto!\n",
      "maininess riddd.\n",
      "\n",
      "\n",
      "the liesway the \"darve that hom \"upon panting himself and rour of\n",
      "egoism who rareful advart they are brook, solizalic rasicacness, and whonbly bring is german--wi\n",
      "Epoch 2/2\n",
      "200285/200285 [==============================] - 77s 383us/step - loss: 1.3030\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"fects of this conjuring back process. th\"\n",
      "fects of this conjuring back process. the strongest the place of the word of the stronger that the more the more intercaurined to the will and the consciousness of the power of the stronger to the stronger and the strongest and stronger the stronges the stronger and consciousness of the moral the more more consciousness of the powerful and and stronger and desire of the strongest to the stronger and long the present and present and cons\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"fects of this conjuring back process. th\"\n",
      "fects of this conjuring back process. the concerning one seems to the to great spirits and delight in whom the \"figrlue of the more into the powerful and enough, grown of the particilaring the instinct for the stands and an any only life in this sharp only a new spirits of the pessimism and light and first experiences in the conscience in the innoticism, the aspect of the spirit, he surest and longing for the best and for the stacked an\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"fects of this conjuring back process. th\"\n",
      "fects of this conjuring back process. the another and consuccion, as foreius called obsigg to grasp of\n",
      "the will, other are from contemppation of scientific\n",
      "riddle spiritness the rater\n",
      "homerywhy for otherwleng those responsibility\", as the foot, will: the whole porton and kind himself charactatic answer. for innecisid in of lorganing the ov"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inceptix\\documents\\github\\cis-3715\\venv\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erstood, by sensationally strength of children and \"greet lost child of the\n",
      "extract of one's own la\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"fects of this conjuring back process. th\"\n",
      "fects of this conjuring back process. thy yeto image. in nobilitated, himin in the inmand exceating plocative\n",
      "path\n",
      "suptain in their seeg, certaining, detervation, as the macue general, an or leate coposing, kle-dilies man does not he conctalls which form certing\n",
      "view adopt with his\n",
      "poor all barberly\n",
      "will.\" ave: whe\n",
      "simplisable \"wistoed, that the\n",
      "origin in that \"circled, thouer expliciable for eth\n",
      "refiners--signs\"--willnt intelleche to c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17370d36c18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model\n",
    "Since it is time consuming to train this LSTM model with CPU for more epochs, we provided a pre-trained model which is trained on GPU for 100 epochs. Use the following code to check how coherency the model is.\n",
    "\n",
    "It requires <b>h5py packages</b>, please install it to test the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained model...\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"from an anguish with which no other is t\"\n",
      "from an anguish with which no other is the subjection of the spectaders that the sense of the subjection of the the consciousness, and a things of the more and such a present to the sense of the subjection of the present is a subjection of the seech of the serve to the subjection of the subjection of the sense of the subjection and something the sense of the subjection of the superfection of the present the subjection of the serve to th\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"resent the subjection of the serve to th\"\n",
      "resent the subjection of the serve to the unto far as a serve of the star to be a the further that you command the exceptional is the classe with scholars of the really noble to the most our little of thought of everything the consciousness and sense of phenomenis, to the practicism, and the everything with the stated and against and all of the present and man to the serve hours all the preserves and servers the thing of the reads to th\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"and servers the thing of the reads to th\"\n",
      "and servers the thing of the reads to this peopons is read of\n",
      "lof\n",
      "pain 8jedlys in aigmeness-say. on the reality, least. as so possible\n",
      "would neted and rece, is too foncefully, the ouls\n",
      "with, when the still\n",
      "piction, bad\n",
      "mind will, as\n",
      "in sharact rests not been use\"---thingy to aweled\n",
      "of animatity he possihanist that fry enough, all beliefought cirviture-hertent for realimistue on the serse of man. how it for little taked \n",
      "socre.apos about\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ow it for little taked \n",
      "socre.apos about\"\n",
      "ow it for little taked \n",
      "socre.apos about. if the\n",
      "commse.ching as have finaral. the lacter man has, israks for romam to this untraesche bnetainastese. the value of prifely a\"germ\n",
      "of the !heshto problem possible, and \"freits. unalled diskbovenealnce. the\n",
      "quance. and ippulted superschtician, and a victurity agceed to\n",
      "give had\n",
      "found and to preserve, prebeerism is favanes!--learcuus,\n",
      "here profosuces they cannugarly matter to this cerbward of\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Load pre-trained model...')\n",
    "from keras.models import load_model\n",
    "model = load_model('shakespear200.h5')\n",
    "\n",
    "\n",
    "def lstm_generate(seed, model):\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        generated += seed\n",
    "        print('----- Generating with seed: \"' + seed + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(seed):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            seed = seed[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "seed = \"from an anguish with which no other is t\"\n",
    "lstm_generate(seed, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: try it to generate baby names\n",
    "-  The baby name data set contains 8000 names. You can download and process the name data set as follows:\n",
    "\n",
    "```python\n",
    "name_path = get_file('names.txt', origin='http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/other/names.txt')\n",
    "with io.open(name_path, encoding='utf-8') as f:\n",
    "    text = f.read() # make it all lowercase \n",
    "    \n",
    "text = text.split()\n",
    "text = ', '.join(text)\n",
    "```\n",
    "\n",
    "Using the baby name data set, answer the following tasks:\n",
    "\n",
    "- Train a LSTM to generate the baby names.\n",
    "- How long does it take to train? How coherent does it sound? \n",
    "- Can you train the LSTM, but for every epoch, shuffle the order of names before call model.fit()? How long does it take to train? Does it improve the coherency?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/other/names.txt\n",
      "450560/443528 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "name_path = get_file('names.txt', origin = 'http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/other/names.txt')\n",
    "\n",
    "with io.open(name_path, encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.split()\n",
    "text = ', '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501788"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb names: 250884\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "maxlen = 20\n",
    "step = 2\n",
    "names = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    names.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb names:', len(names))\n",
    "\n",
    "# Turn these sentances into one-hot encoded vectors\n",
    "## For all words in the sentances, there is a one, else there is a zero in that index of the vector\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(names), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(names), len(chars)), dtype=np.bool)\n",
    "for i, name in enumerate(names):\n",
    "    for t, char in enumerate(name):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(40):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250884/250884 [==============================] - 50s 199us/step - loss: 1.2919\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", Gronberg, Grondahl\"\n",
      ", Gronberg, Grondahl, Grondell, Grondle, Grondle, Grondle, G\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", Gronberg, Grondahl\"\n",
      ", Gronberg, Grondahl, Groncol, Gronco, Gronch, Gronch, Gronc\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", Gronberg, Grondahl\"\n",
      ", Gronberg, Grondahl, Gronds, Gnona, Gron, Grona, Grona, Gro\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", Gronberg, Grondahl\"\n",
      ", Gronberg, Grondahlan, Gronde, Groedj, Croesa, Croffe, Coff\n",
      "Epoch 2/10\n",
      "250884/250884 [==============================] - 49s 196us/step - loss: 0.8793\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", Worster, Worth, Wo\"\n",
      ", Worster, Worth, Worther, Worther, Worther, Worthard, Worth\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", Worster, Worth, Wo\"\n",
      ", Worster, Worth, Worther, Worther, Worthett, Worthaff, Wort\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", Worster, Worth, Wo\"\n",
      ", Worster, Worth, Worthen, Worthren, Worther, Worthia, Worth\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", Worster, Worth, Wo\"\n",
      ", Worster, Worth, Wortheff, Worthughreaut, Wortesson, Wortez\n",
      "Epoch 3/10\n",
      "250884/250884 [==============================] - 47s 188us/step - loss: 0.8199\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" Mckamey, Mckane, Mc\"\n",
      " Mckamey, Mckane, Mckano, Mckano, Mckanous, Mckaro, Mckaro, \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" Mckamey, Mckane, Mc\"\n",
      " Mckamey, Mckane, Mckanek, Mckanora, Mckany, Mckaney, Mckans\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" Mckamey, Mckane, Mc\"\n",
      " Mckamey, Mckane, Mckamsel, Mckalley, Mckalerma, Mckale, Mck\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" Mckamey, Mckane, Mc\"\n",
      " Mckamey, Mckane, Mckaurgetta, Mckaugreoure, Mckaglanzio, Mc\n",
      "Epoch 4/10\n",
      "250884/250884 [==============================] - 49s 195us/step - loss: 0.7869\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"gdill, Coger, Cogges\"\n",
      "gdill, Coger, Cogges, Coggstrom, Coggstrom, Coggstein, Coggs\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"gdill, Coger, Cogges\"\n",
      "gdill, Coger, Cogges, Coggstrom, Cogin, Cogor, Cogrin, Cogur\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"gdill, Coger, Cogges\"\n",
      "gdill, Coger, Cogges, Coggeson, Cogget, Coggs, Cogurst, Cogu\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"gdill, Coger, Cogges\"\n",
      "gdill, Coger, Cogges, Cogge, Coggarg, Coggner, Coggy, Coggyw\n",
      "Epoch 5/10\n",
      "250884/250884 [==============================] - 49s 195us/step - loss: 0.7642\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"an, Strawn, Strawser\"\n",
      "an, Strawn, Strawser, Straw, Strawbrooks, Strawden, Strawder\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"an, Strawn, Strawser\"\n",
      "an, Strawn, Strawser, Straw, Strawbrooks, Strawden, Straw, S\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"an, Strawn, Strawser\"\n",
      "an, Strawn, Strawser, Straltz, Stralttzer, Straltz, Strallz,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"an, Strawn, Strawser\"\n",
      "an, Strawn, Strawser, Strawel, Strawd, Strawmer, Strow, Stro\n",
      "Epoch 6/10\n",
      "250884/250884 [==============================] - 49s 196us/step - loss: 0.7472\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Hodgins, Hodgkin, Ho\"\n",
      "Hodgins, Hodgkin, Hodgner, Hodgs, Hodgris, Hodgriss, Hodgris\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Hodgins, Hodgkin, Ho\"\n",
      "Hodgins, Hodgkin, Hodghall, Hodgs, Hodgridge, Hodguis, Hodgw\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Hodgins, Hodgkin, Ho\"\n",
      "Hodgins, Hodgkin, Hodia, Hodich, Hodich, Hodicek, Hodich, Ho\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Hodgins, Hodgkin, Ho\"\n",
      "Hodgins, Hodgkin, Hodgner, Hodkine, Hodkinschner, Hodlichale\n",
      "Epoch 7/10\n",
      "250884/250884 [==============================] - 49s 197us/step - loss: 0.7331\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Spielmann, Spielvoge\"\n",
      "Spielmann, Spielvogep, Spiello, Spiello, Spiell, Spielle, Sp\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Spielmann, Spielvoge\"\n",
      "Spielmann, Spielvogem, Spiens, Spienover, Spiener, Spier, Sp\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Spielmann, Spielvoge\"\n",
      "Spielmann, Spielvogepaudeier, Speris, Spriffe, Spricher, Spr\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Spielmann, Spielvoge\"\n",
      "Spielmann, Spielvoger, Spiemito, Spientz, Spie, Spier, Speir\n",
      "Epoch 8/10\n",
      "250884/250884 [==============================] - 50s 199us/step - loss: 0.7190\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"Whicker, Whidby, Whi\"\n",
      "Whicker, Whidby, Whidden, Whide, Whidel, Whidell, Whidell, W\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"Whicker, Whidby, Whi\"\n",
      "Whicker, Whidby, Whid, Whida, Whida, Whida, Whidell, Whidell\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"Whicker, Whidby, Whi\"\n",
      "Whicker, Whidby, Whidccas, Whidchill, Whidder, Whidfield, Wh\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"Whicker, Whidby, Whi\"\n",
      "Whicker, Whidby, Whiddg, Whidger, Whidhams, Whidg, Whidkama,\n",
      "Epoch 9/10\n",
      "   128/250884 [..............................] - ETA: 1:01 - loss: 0.7372"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\inceptix\\documents\\github\\cis-3715\\venv\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250884/250884 [==============================] - 48s 191us/step - loss: 0.7091\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"son, Hudgens, Hudgin\"\n",
      "son, Hudgens, Hudging, Hudgle, Hudgo, Hudgo, Hudgo, Hudgo, H\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"son, Hudgens, Hudgin\"\n",
      "son, Hudgens, Hudgington, Hudins, Hudis, Hudke, Hudder, Hude\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"son, Hudgens, Hudgin\"\n",
      "son, Hudgens, Hudgin, Hudgo, Hudbozno, Hudda, Hudda, Huddal,\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"son, Hudgens, Hudgin\"\n",
      "son, Hudgens, Hudgin, Hudgo, Hudgoodt, Hudgo, Hudk, Hudge, H\n",
      "Epoch 10/10\n",
      "250884/250884 [==============================] - 48s 192us/step - loss: 0.7002\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \", Hardison, Hardiste\"\n",
      ", Hardison, Hardister, Hardman, Hardo, Hardon, Hardon, Hardo\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \", Hardison, Hardiste\"\n",
      ", Hardison, Hardister, Hardley, Hardo, Hardow, Hardrey, Hard\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \", Hardison, Hardiste\"\n",
      ", Hardison, Hardister, Hardwood, Hare, Hareer, Harehart, Har\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \", Hardison, Hardiste\"\n",
      ", Hardison, Hardister, Hardoy, Hardole, Hardney, Hardureth, \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17370d368d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "%timeit -n1 -r1\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
